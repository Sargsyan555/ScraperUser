import { Injectable, Logger } from '@nestjs/common';
import { Cron, CronExpression } from '@nestjs/schedule';
import axios from 'axios';
import { XMLParser } from 'fast-xml-parser';
import * as cheerio from 'cheerio';
import * as fs from 'fs';
import * as path from 'path';
import * as xlsx from 'xlsx';

type ScrapedProduct = {
  url: string;
  title: string;
  price: string;
  article: string;
};

@Injectable()
export class ScraperServiceDvPt {
  private readonly sitemapUrl = 'https://dv-pt.ru/sitemap.xml';
  private readonly logger = new Logger(ScraperServiceDvPt.name);

  constructor() {
    this.fetchAndSaveUrls().catch((err) =>
      this.logger.error('Initial download failed:', err),
    );
    this.scrapeAndExport().catch((err) =>
      this.logger.error('Initial scrap from .json  failed:', err),
    );
  }

  @Cron(CronExpression.EVERY_3_HOURS)
  async handleCron() {
    this.logger.log('–ó–∞–ø—É—â–µ–Ω –ø–ª–∞–Ω–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥ dv-pt');
    try {
      await this.fetchAndSaveUrls();
      await this.scrapeAndExport();
      this.logger.log('‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∏ —ç–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ.');
    } catch (error) {
      this.logger.error('‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –ø–ª–∞–Ω–æ–≤–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞:', error);
    }
  }

  private async fetchAndSaveUrls(): Promise<void> {
    this.logger.log(`–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –∏–∑ sitemap: ${this.sitemapUrl}`);
    const response = await axios.get(this.sitemapUrl);
    const parser = new XMLParser();
    const parsed = parser.parse(response.data);
    const allUrls = parsed.urlset.url.map((item) => item.loc);

    const productUrls = allUrls.filter((url) => url.includes('/shop/goods/'));

    const filePath = path.join(process.cwd(), 'dv-pt.json');
    fs.writeFileSync(filePath, JSON.stringify(productUrls, null, 2), 'utf-8');

    this.logger.log(`–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ ${productUrls.length} —Å—Å—ã–ª–æ–∫ –≤ ${filePath}`);
  }

  private async scrapeAndExport(): Promise<void> {
    const filePath = path.join(process.cwd(), 'dv-pt.json');
    const urls: string[] = JSON.parse(fs.readFileSync(filePath, 'utf-8'));

    const results = await this.processAll(urls, 8, 500, 300);

    const worksheet = xlsx.utils.json_to_sheet(results);
    const workbook = xlsx.utils.book_new();
    xlsx.utils.book_append_sheet(workbook, worksheet, 'Products');

    const outputFilePath = path.join(
      process.cwd(),
      '/src/telegram/scraper',
      'dvpt.xlsx',
    );
    xlsx.writeFile(workbook, outputFilePath);

    this.logger.log(`üì¶ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: ${outputFilePath}`);
  }

  private async scrapePage(url: string, attempt = 1): Promise<any> {
    const delay = (ms: number) => new Promise((res) => setTimeout(res, ms));

    try {
      const response = await axios.get(url, {
        headers: {
          'User-Agent':
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',
          'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
          Referer: 'https://dv-pt.ru/',
          Accept:
            'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        },
        timeout: 15000,
      });

      const $ = cheerio.load(response.data);
      const title = $('h1').first().text().trim();
      const price = $('.price .strong').first().text().trim();
      const article = $('.values span').first().text().trim();

      return { url, title, price, article };
    } catch (error) {
      if (error.response?.status === 403 && attempt < 3) {
        this.logger.warn(
          `‚ö†Ô∏è 403 Forbidden. –ü–æ–≤—Ç–æ—Ä (${attempt + 1}) –¥–ª—è: ${url}`,
        );
        await delay(50000);
        return this.scrapePage(url, attempt + 1);
      }

      this.logger.error(`‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ ${url}: ${error.message}`);
      return { url, title: '', price: '', article: '', error: error.message };
    }
  }

  private async processAll(
    urls: string[],
    concurrency = 7,
    perRequestDelay = 500,
    startDelayStep = 300,
  ): Promise<any[]> {
    const results: ScrapedProduct[] = [];
    let index = 0;
    const delay = (ms: number) => new Promise((res) => setTimeout(res, ms));

    async function worker(workerId: number) {
      await delay(workerId * startDelayStep);

      while (true) {
        const currentIndex = index++;
        if (currentIndex >= urls.length) break;

        const url = urls[currentIndex];
        console.log(
          `üîÑ [Worker ${workerId + 1}] ${currentIndex + 1}/${urls.length}: ${url}`,
        );

        const result = await this.scrapePage(url);
        results.push(result);

        await delay(perRequestDelay);
      }
    }

    const boundWorker = worker.bind(this); // –≤–∞–∂–Ω—ã–π –º–æ–º–µ–Ω—Ç!
    const workers = Array.from({ length: concurrency }, (_, i) =>
      boundWorker(i),
    );

    await Promise.all(workers);
    return results;
  }
}
